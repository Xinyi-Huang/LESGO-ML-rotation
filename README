Copyright (C) 2009-2016  Johns Hopkins University

Copying and distribution of this file, with or without modification,
are permitted in any medium without royalty provided the copyright
notice and this notice are preserved.  This file is offered as-is,
without any warranty.

Written by:

  Stuart Chester
  Jason Graham <jgraha8@gmail.com>
  Claire Verhulst <cverhuls@gmail.com>
  Richard Stevens <r.j.a.m.stevens@gmail.com>
  Tony Martinez <tony.mtos@gmail.com>
  Joel Bretheim <jbretheim@gmail.com>
  Xiang Yang <xiangyang@jhu.edu>
  Carl Shapiro <crshapiro@gmail.com>

================================================================================
                    DISCLAIMER
================================================================================

"Please keep in mind that many parts of the code were developed without a clear
idea of exactly where the program was headed, since various ideas are tried and
discarded as part of the research process.  This means that code organization is
not optimal in some spots, and that not all options are fully implemented.  My
advice is ALWAYS read whatever parts of the code you are using to find out what
they really do." - SC

================================================================================
                    OVERVIEW
================================================================================

The code lesgo solves the filtered N-S equations in the high-Re limit on a
Cartesian mesh. Originally lesgo was designed to simulate flow in the
atmospheric boundary layer but has been extended and used to simulate flow over
tree canopies, wall mounted cubes, wind turbine arrays, and others. At its core 
is the LES flow solver. Built on top of the solver are modules that provide 
additional functionality such as an immersed boundary method, wind farm modeling, 
and others. The following is a list of the major components of lesgo:

  * LES Flow Solver

  * Level Set Immersed Boundary Method

  * Wind farm modeling 

  * Concurrent precursor simulation

Each of these components are discussed in detail in the following sections.

================================================================================
                    DEPENDENCIES
================================================================================

The following dependencies are required in order to compile lesgo:

  * Fortran 95 compiler: 

    Most modern compilers will work fine, but the compiler must support c 
    preprocessor directives. Compilers that have been used successfully are 
    gfortran and ifort
    
  * CMake
  
    lesgo is designed to be built using CMake (kitware.com/cmake). Compilation 
    options for CMake are in "CMakeLists.txt".
    
  * FFTW3
  
    As a spectral code, lesgo uses many Fourier transforms. lesgo relies on the 
    FFTW3 library (http://www.fftw.org/) to do these transforms.

--------------------------------------------------------------------------------
                    OPTIONAL
--------------------------------------------------------------------------------

  * MPI implementation:

    A MPI implementation will be required for MPI support. The code has been
    developed and used successfully with MPICH2 and OPENMPI:
    http://www.mcs.anl.gov/research/projects/mpich2/
    http://www.open-mpi.org/
    
  *  HDF5 and CGNS
    
    lesgo supports to use of the cgns/hdf5 output standard. To enable this 
    feature, the hdf5 and cgns libraries must be installed.
    See INSTALLATION_NOTES for installation instructions for these libraries

================================================================================
                GETTING STARTED
================================================================================

To get started, you'll need to configure two files: "CMakeLists.txt" and
"lesgo.conf". The file "CMakeLists.txt", in addition to compiler settings, has
preprocessor flags for compiling in different functionality (MPI support,
immersed boundary support, etc.). The file "lesgo.conf" is the input file for
the code which you'll set your case up. It is commented fairly well and is
hopefully self-descriptive enough to get started.

Once you configure "CMakeLists.txt" for your system compile lesgo using:

  ./build-lesgo

The compilation will occur in the subdirectory ./bld/ If major changes to 
"CMakeLists.txt" are made, the above command may fail. In this case, remove
the build directory using:

  rm -r bld

The resulting executable name will reflect the functionality that you have
configured into the executable. For instance, for serial lesgo with no additional
bells and whistles the executable will be called "lesgo"; if you turned on MPI
support it will be called "lesgo-mpi". Once you have an executable there is no
need to recompile for running different cases - this is done using "lesgo.conf".

To configure your case simply update "lesgo.conf" appropriately. As mentioned
above, "lesgo.conf" should be commented well enough to get started. If anything
is unclear, feel free to contact one of the authors listed at the top of the
file.

When running, lesgo will output data to the directory "output". Also during
execution, lesgo writes information to screen based on the value "wbase" in
"lesgo.conf". This information includes time step information, CPU time,
velocity divergence, etc and is helpful for monitoring the case. Another
important file for monitoring the simulation is the file "output/check_ke.dat"
which has the average kinetic energy of the field. Typically the kinetic energy
will decay during the spinup period and asymptote once stationarity is
reached. This gives a good indicator for determining when to start taking
statistics.

The code count the number of timesteps in a series of simulations, the number of 
timesteps at the end of each simulation is stored in total_time.dat to make sure 
that the numbering of all time-dependent files generated by the code are continuous 
as expected when the simulation would have been completed within one run. The total 
time counter is jt_total. Some variables need to be calculated at the beginning of 
a simulation, which requires the local jt counter in lagrange_Ssim.f90 and 
lagrange_Sdep.f90. In order to reset the counter one needs to change the file 
total_time.dat.

================================================================================
                LES FLOW SOLVER
================================================================================

The LES flow solver is a pseudo-spectral solver in which spectral discretization
is applied in the longitudinal and spanwise directions (x and y, respectively)
and in the vertical (z) direction 2nd order finite differences are applied. The
solver solves the filtered N-S equations in the high-Re limit in which molecular
viscosity effects are generally neglected. An eddy viscosity closure model is
applied for the sub-grid scale stresses. Boundary conditions along the x-y
perimeter are periodic where a log-law stress is applied at the bottom surface;
a stress free condition is applied at the top (channel center) boundary. Solid
objects are represented using a level set immersed boundary method. In addition
to the bottom wall, solid surfaces represented by the immersed boundary method
also apply a log-law stress boundary condition. For temporal integration the
explicit 2nd order Adams-Bashforth method is used. Time stepping occurs using
either a static time step or dynamic CFL based dynamic time step. Continuity is
preserved by solving the pressure Poisson equation using a direct TDMA solver.

--------------------------------------------------------------------------------
                MPI DOMAIN DECOMPOSITION
--------------------------------------------------------------------------------

The LES flow solver is parallelized with MPI. The flow domain is evenly divided
in the vertical (z) direction between the MPI processes. Each MPI-process has
its "own" z-levels indexed by k = 1 to nz-1. The z-levels k = 0 and k = nz are
overlap nodes and are only for holding data that has been copied from the
process below or above the current one. For the main flow solver, the only
overlap data that is required is for the vertical derivatives. Since the
spectral discretization occurs along the slab planes no additional overlap data
is required. In the future, it may be advantageous to also decompose the slabs
along the y-direction and utilize the parallel FFTW3 MPI library in order to
increase scalability for large domains. 

For the pressure solver, a pipelining technique was chosen to parallelize the
TDMA, since it is simple and there isn't really any better options for
parallelizing the solution of many small tridiagonal systems. The best size of
the chunks to send "down the pipeline" can be controlled via the variable
"chunksize", and WILL depend on the computer hardware and simulation size. It
would probably advantageous to add a routine that adjusts the chunk size
automatically by making speed measurements at the beginning of the simulation.

--------------------------------------------------------------------------------
                    VARIABLE LOCATIONS
--------------------------------------------------------------------------------

The code employs a staggered grid along the vertical direction. This means that
not all variables are stored on the same grid. We call the two grids the 'uv'
grid and the 'w' grid, where the grids are distinguished based on the location
of the velocity components. The only difference is that the 'uv' grid is shift
+dz/2 in the vertical direction, where dz is the vertical grid spacing. Also,
the 'w' grid conforms to the physical domain such that the first grid point lies
at z=0 and the last grid point at z=L_z where L_z is the domain height.

The following is a list of the variables and the grid they exist on. The names
are the same as found in lesgo.

        Variables                       'uv' grid       'w' grid
 =======================                =========       =========
 u, v, p                                    X
 w                                                          X
 dudx, dudy, dvdx, dvdy                     X
 dudz, dvdz                                                 X
 dwdx, dwdy                                                 X
 dwdz                                       X
 dpdx, dpdy                                 X
 dpdz                                                       X
 txx, txy, tyy, tzz                         X
 txz, tyz                                                   X
 RHSx, RHSy                                 X
 RHSz                                                       X
 divtx, divty                               X
 divtz                                                      X

When writing data to file the variables are interpolated to the same grid.

================================================================================
                    DATA OUTPUT
================================================================================

The output data from lesgo contains two types: 1) restart data and 2)
visualization data. Both types are described here. It should be noted that the
file names for serial runs will only be listed. In the case of MPI runs some
files will be appended with '.c<id>' where <id> is the z-ordered MPI rank used
in the simulation. For files where this is applicable, it will be noted by the
term 'MPI appendix'

Below are listed the output files along with a description
of their contents. These file use the 'MPI appendix'

  1) vel.out : Contains the core part of the restart data for lesgo. In this
               file the velocity, right hand side of the momentum equation, and
               several SGS variables are store. Essentially all the data
               required from the previous time step or steps is located in these
               files.

  2) tavg.out : Contains the time averaged data from the previous simulation. In
                this file the running averages of the velocity, Reynolds stress,
                etc are store here and are used to maintain the averaging
                between simulations.

  3) spectra.out : Contains the time averaged spectra data from
                   the previous simulation.

The visualization data is located in the directory "output". The "output"
directory is created by making a 'mkdir' system call and should work fine on all
Unix based systems. By default these file are written in binary formatted files
using direct fortran write calls. The output to this directory depends on the
settings in the OUTPUT block in the input file "lesgo.conf". In this section we
will discuss the output from the core LES solver of lesgo. All data output from
various modules will be discussed in their respective sections.

  A) Time Averaged Data

    1) Entire domain (these file use the 'MPI appendix')

      * vel_avg.dat         : Mean velocity field
      * vel2_avg.dat        : Mean of square products of the velocity field
      * ddz_avg.dat         : Mean of vertical gradients
      * force_avg.dat       : Mean field of modeled forces (IBM, turbines, RNS, etc)
      * tau_avg.dat         : Mean of the sub-grid scale stress tensor
      * rs.dat              : Reynolds stresses

  2) Averaged over z-planes

    * vel_zplane_avg.dat    : Mean velocity field
    * vel2_zplane_avg.dat   : Mean of square products of the velocity field
    * ddz_zplane_avg.dat    : Mean of vertical gradients
    * force_zplane_avg.dat  : Mean field of modeled forces (IBM, turbines, RNS, etc)
    * tau_zplane_avg.dat    : Mean of the sub-grid scale stress tensor
    * rs_zplane.dat         : Reynolds stresses
    * cnpy_zplane.dat       : Canopy (dispersive) stresses
    * cs_opt2_zplane.dat    : Square of Smagorinsky coefficient from SGS modeling

B) Domain Data (these files use the 'MPI appendix')

  * vel.<timestep>.dat      : Instantaneous velocity field at time step <timestep>

C) Sampled Point Data

  * vel.x-<xloc>.y-<yloc>.z-<zloc>.dat :
    Instantaneous velocity sampled at point (<xloc>,<yloc>,<zloc>) 

D) Data along x-planes (these files use the 'MPI appendix')

  * vel.x-<xloc>-<timestep>.dat :
    Instantaneous velocity sampled at x location <xloc> for the time step <timestep>

E) Data along y-planes (these files use the 'MPI appendix')

  * vel.y-<yloc>-<timestep>.dat :
    Instantaneous velocity sampled at y location <yloc> for the time step <timestep>

F) Data along z-planes

  * vel.z-<zloc>-<timestep>.dat : 
    Instantaneous velocity sampled at z location <zloc> for the time step <timestep>
 
================================================================================
                LEVEL SET IMMERSED BOUNDARY METHOD
================================================================================

The immersed boundary technique used in lesgo is described in this section.

--------------------------------------------------------------------------------
                    OVERVIEW
--------------------------------------------------------------------------------

For representing solid objects in the flow domain, the level set immersed
boundary method is used. All objects are represented using a level set or signed
distance function (phi) where phi = 0 on the surface, phi < 0 inside objects and
phi > 0 otherwise. For each grid point in the computational domain the minimum
distance to any one of the objects in the domain must be computed and assigned
to the variable phi using a preprocessing program. The code will then use phi to
compute the surface normals which is then used to apply the log-law stress
within a small "band" along a the surface boundary. For all grid points on or
inside of the surface the velocity is forced to zero using a direct forcing
approach.

--------------------------------------------------------------------------------
                    USAGE
--------------------------------------------------------------------------------

To use the level set modules in a simulation, a file "phi.out" containing the
signed distance function data must be created first.  This must be generated
with a separate program or using the built-in  "trees_pre_ls" functionality. 
It is recommended making the signed distance function exact if possible, or at 
least sampled at a higher resolution than the computational grid.  There are many 
adjustable parameters within the level set module that control how the boundary 
conditions at the level set surface are applied. These are listed in the input file 
lesgo.conf within the LEVEL_SET block. If you encounter problems (e.g., kinks in 
velocity profiles), try adjusting some of these (e.g., length scale parameters that
control how close a given point must be to the surface before an certain action
is taken). Just make sure to read all code associated with some of these
parameters, since some are experimental, and may not be ready for "prime time".

The variables "nphitop", "nphibot", etc, control how many extra z-levels are
copied between MPI processes when determining boundary conditions (at top and
bottom of the process-local domain).  The values to use here are geometry
dependent, and it is pretty hard to determine in advance exactly what they
should be. These values should be as small as possible so the MPI transfers
involve the least amount of data. Right now, my approach has been to pick some
initial values and when the code fails, then increase the values.  the good news
here is that all the code using these is manually bounds-checked, so if an
out-of-bounds reference is made, the code should always die with an error
message saying which parameter is the problem and offer a suggestion as to what
a better value would be. To adjust these parameters add to the LEVEL_SET input
block a line that has the variable that needs to be modified. For example, if
you run into an error regarding nphitop, add to the LEVEL_SET block a line such
as:

  nphitop = 3

These are omitted by default from lesgo.conf since the defaults seem to work in
general.

Another weakness is that the number of extra z-levels is the same for all
processes, when for maximum efficiency, they really should be allowed to differ.
It should be possible to have the code auto-dimension the "nphitop", etc, for
each process.  On the other hand, this means that those processes that finish
their boundary conditions faster that the other processes will have to wait, so
unless they are given something useful to do, it is probably not worth the
effort.

--------------------------------------------------------------------------------
            CREATING TREES WITH "trees_pre_ls"
--------------------------------------------------------------------------------

The built-in "trees_pre_ls" functionality may be used for generating the "phi" 
field for the level set module. This generates fractal trees with either round or 
square branches. In addition to trees, it may also be used to easily generate an 
array of wall mounted cubes which is commonly used as one of the test cases for lesgo.

To use this feature, simply enable the use_trees the LVLSET block and write a "trees.conf"
file. The file "trees.conf" is the input file for trees_pre_ls ("lesgo.conf" is
also required by trees_pre_ls). An example "trees.conf" is:

--------------begin trees.conf-----------
# This is a trees configuration file
# this is a comment, it begins with a #

n_tree = 1

# see trees_setup_ls.f90 to see what each of these parameters does
tree = {
  n_gen = 2
  n_sub_branch = 4
  l = 0.3125
  d = 0.125
  #x0 = 0.5, 0.5, 0.0078125
  x0 = 0.5, 0.5, 0.0
  taper = 0.
  ratio = 0.48
  rel_dir = -0.4924038763,-0.8528685321,-0.1736481773,
0.0000000000,0.0000000000,1.0000000000,
-0.4924038763,0.8528685320,-0.1736481773,
0.9848077530,0.0000000000,-0.1736481773
  root_height = 0.75, 1.0, 0.75, 0.75
  twist = -90.0, 0.0, -90.0, -90.0
  #trunk_twist = 90.0
  max_res_gen=1
}
--------------end trees.conf--------------

This creates a tree structure with 2 generations of branches (the trunk counts
as generation zero), with each branch having 4 sub-branches.  The length of each
the trunk is 0.3125 and the diameter of the trunk is 0.125.  If the add_cap
option is true, then the actual length of the tree trunk will be l + d/2 =
0.3125 + 0.125/2 = 0.375.  The center of the base of the trunk is at x0 (given
as x,y,z coordinates).  The branches are not tapered.  The ratio of lengths
between a branch and each of its sub-branches is 0.48.  The directions of the
sub-branches, relative to the parent-branch coordinate system are sub-branch 1:
(-0.49, -0.85, -.17) sub-branch 2: (0, 0, 1) sub-branch 3: (-0.49, 0.85, -0.17)
sub-branch 4: (0.98, 0, -0.17) To see how the branch-local coordinate systems
are defined, see trees_setup_ls.f90. Three sub-branches are placed 75 % of the
way along the parent branch (root_height), and one is at the top of the parent
branch.  Each sub-branch has a twist about its own branch axis applied to it.
The trunk can be twisted separately, but this line has been commented out.  The
maximum resolved generation(max_res_gen) used in RNS is generation 1, note that
there n_gen = 2 is one more than the last resolved generation.  The is mean that
generation two are the unresolved RNS branches.  Even if you want to simulate
more that one unresolved branch generation, right now the code expects
max_res_gen + 1 = n_gen.  Note that although the relations between a branch and
its sub-branches are defined in trees.conf, iterated function systems (IFS) ARE
NOT USED to describe the trees.  Instead, a sub-branch is defined only by
reference to its parent branch.

The level set function required by the level set routines will be calculated at 
before the first timestep of each lesgo run.
 
================================================================================
            CONCURRENT PRECURSOR SIMULATION
================================================================================

lesgo has the ability to use an inflow condition instead of the standard
periodic condition. The inflow can either be a uniform or laminar inflow or it
can be turbulent inflow generated from a precursor simulation. In this section,
the concurrent precursor simulation (CPS) module which is used to provide the
precursor data in a concurrent framework is discussed.

--------------------------------------------------------------------------------
                    OVERVIEW
--------------------------------------------------------------------------------

Traditionally, when using inflow data from a precursor simulation, it is
required that a complete simulation be conducted before the target simulation
can be performed. During the precursor simulation the inflow data would be
sampled and periodically written to file. Subsequently, this inflow data would
then be read in by the target simulation. While this approach is conceptually
simple, it does have several drawbacks such as the following:

  1) Only one simulation is performed at a given time, i.e., must have the data
     from the precursor simulation before the target simulation can be executed.

  2) May require significant disk space for large simulations.

  3) Requires significant I/O which may be a hindrance for good computational
     efficiency.

  4) Must coordinate the precursor simulation with the target simulation to
     ensure enough inflow data for the target simulation.

To alleviate these issues, the CPS module performs the precursor simulation
concurrently with the target simulation. Conceptually the approach is the same,
except of writing the sampled inflow data from the precursor simulation to file
it is copied in memory directly to the target simulation using MPI. Since both
simulations are executed simultaneously, there is no waiting for the precursor
simulation to complete and direct memory copies remove the I/O overhead both in
speed and storage space.

--------------------------------------------------------------------------------
                IMPLEMENTATION
--------------------------------------------------------------------------------

In the CPS module the precursor simulation is conducted in the 'producer' domain
and is called the 'red' domain. The target simulation occurs in the 'consumer'
domain which is labeled the 'blue' domain. MPI communication between these
domains and within themselves is controlled by defining appropriate MPI
communicators. 

At the start of the simulation the "MPI_COMM_WORLD" communicator is split into
two local communicators "localComm" where the 'red' and 'blue' each have their
own. lesgo then takes the local communicator and create a communicator called
"comm" using the MPI Cartesian topology functions. The communicator "comm" is
used for all MPI communication within the 'red' or 'blue' domains and is the
intracommuncator for these domains. The "comm" communicator is also used for
standard MPI with CPS turned off, which results in no special treatment for
point-to-point communication when using CPS.

For communication between the 'red' and 'blue' domain an intercommunicator
"interComm" is created. It essentially builds a communication bridge for each
process in the 'red' domain to communicate with the corresponding process in the
'blue' domain.

When using the CPS module, the simulation is executed as multiple program,
multiple data (MPMD) paradigm. Therefore, the simulation is launched with "N"
process and "N/2" get assigned to the 'red' domain and the other "N/2" are
assigned to the 'blue' domain. Within the global "MPI_COMM_WORLD" communicator,
the domains have global ranks assigned to them such that 0 to N/2-1 is given to
the 'red' domain and N/2 to N-1 is assigned to the 'blue' domain. Once the local
communicator "comm" is created each domain has the local ranks 0 to N/2-1
assigned to each of the processes. The intercommunicator then takes these local
ranks and maps rank "n" from 'red' to rank "n" in 'blue' creating the bridge for
copying the inflow data.

There is a capability to shift the domain in order to eliminate the streaks. 
This option will shift the domain to the side of the precursor domain. 
--------------------------------------------------------------------------------
                USAGE
--------------------------------------------------------------------------------

The first step is to build in the CPS support into lesgo by setting

  USE_MPI = yes
  USE_CPS = yes

If you want the streaks option set (only on the precursor or red domain)
  USE_STREAKS=yes

in "CMakeLists.txt". You will need to build in CPS support for both the 'red' and
'blue' domain executables. Other support may be built for required functionality
but at a minimum CPS is needed. An example for this is the setup for developing
flow over a wind farm. In the 'red' domain we'd have standard boundary layer
flow so we'd set

  USE_MPI = yes
  USE_CPS = yes

as above and build the executable. Then we'd also need to include the wind
turbines module for the 'blue' domain so we'd set

  USE_MPI = yes
  USE_CPS = yes
  USE_TURBINES = yes

and build lesgo.

Once the executables are built you can then setup your cases. For now we'll call
the executable for the 'red' domain "lesgo-red" and the one for the 'blue'
domain "lesgo-blue". Each domain will need it's own run directory so you'll have
to create these; we'll call these directories "red" and "blue". The executables
should then be placed in their respective run directory. A copy of the input
file "lesgo.conf" will have to be place in each of the run directories. 

Now the input files have to be configured. The number of processors should be
set what will be used for each domain. So, for example, if each domain will use
4 processes, then

  nproc = 4

in "lesgo.conf" for both cases. When submitting the job, you have to request the
total number of processes being used. Continuing with the example you have to
request 8 process if the 'red' and 'blue' domains use 4 each. The next important
setting is the inflow flag "inflow". For the red domain it must be set to

  inflow = .false.

where the 'blue' domain will use

  inflow = .true.

To provide the inflow condition while numerically still using periodic boundary
conditions, a fringe method is applied. The fringe method is a well established
technique for forcing the velocity field to the desired, sampled field over a
small region called the fringe region. There are several settings which control
the location and size of this fringe region. 

One constraint of the CPS module is that the grid spacing of the 'red' and
'blue' domains must match. The domain lengths can be different, but the grid
spacing must be the same to ensure accurate results. Another constraint is that
because these simulation are synchronized, the same value for "dt" must be
specified unless dynamic time stepping is used, then the same CFL value should
be used for both domains.

The specifics on launching the simulation depends on which MPI implementation is
used. This is discussed below, assuming we are using executables named
"lesgo-red" and "lesgo-blue" and run directories "red" and "blue" for the "red"
and "blue" domains, respectively, with a total of "N" processes.

  A) MPICH2 launch command

     mpiexec  -wdir red -np <N/2> ./lesgo-red : -wdir blue -n <N/2> ./lesgo-blue > lesgo.out
   
  in blue pressure gradient is 0

When running, all diagnostic information is written to standard out with no
specific order.

================================================================================
                                WIND-TURBINES
================================================================================

lesgo also has the ability to simulate an array of wind-turbines.  To use this
feature set USE_TURBINES to true in "CMakeLists.txt".  The turbine-specific settings 
are found in "lesgo.conf".  These will be discussed in detail below.  The 
subroutines associated with the turbine calculations are found in the files 
"turbines_base.f90" and "turbines.f90".  Note: this feature is fully compatible 
with the MPI domain discretization.

--------------------------------------------------------------------------------
                            THE TURBINE MODEL
--------------------------------------------------------------------------------

Each turbine is represented as a drag disk with a force that depends on the 
velocity at the disk (averaged in time and space).  This force is distributed 
across several grid points that together represent the turbine.  For large arrays 
the coarse grid resolution does not allow for the modeling of individual 
blades. No tangential forces are applied to the flow, though it may prove
beneficial to include these in the future (see Meyers 2010).

The turbine force is given by 
    F = -0.5 rho Ct' <Ud>^2 A        [Calaf, eqn 18]
        rho is the fluid density
        Ct' is the modified thrust coefficient
        <Ud> is the disk- and time-averaged velocity 
        A is the disk frontal area

The velocity at the disk is averaged in time using a one-sided exponential 
filter.  The filtering operation is as follows:
    G(t) = integral_(-infinity)^(t) { g(t') * exp((t'-t)/T) / T } dt'
        G is the time-averaged quantity (aka <Ud>)
        g is the instantaneous quantity (aka Ud)
        T is the time scale of the filter
Taking the derivative with respect to time and using a first-order 
discretization for the time derivative we find
    G(t+dt) = (1-eps) * G(t) + eps * g(t+dt)
        eps = (dt/T) / ( 1 + dt/T )
        dt is the simulation time step
        T is the time scale of the filter
which is used to update the time-averaged quantity as the simulation progresses.

The force is distributed across grid points using an indicator function which
is determined during code initialization.  The grid points that fall within 
each turbine radius are located.  To avoid Gibbs phenomenon with sharp gradients,
this indicator function (currently 1 inside and 0 outside a turbine) is smoothed
with a Gaussian filter.  To avoid calculating too many convolutions, the spatial
extent of this filtering process is limited to a set number of grid points past 
the turbine radius.  Also, to limit the spatial extent of each turbine, a
minimum allowable value of the indicator function is specified.  Each grid point 
with a non-zero indicator function applies a force on the flow.


REFERENCES:
Calaf, Meneveau, and Meyers. "Large eddy simulation study of fully developed
wind-turbine array boundary layers." Physics of Fluids 22 (2010).
http://dx.doi.org/10.1063/1.3291077

Meyers and Meneveau.  "Large eddy simulations of large wind-turbine arrays in 
the atmospheric boundary layer." AIAA Paper No. 2010-827.

--------------------------------------------------------------------------------
                    TURBINE SETTINGS IN "lesgo.conf"
--------------------------------------------------------------------------------

The first group of settings specifies the wind-turbine array geometry and
orientation.  The user can set the number of turbines in each direction as well
as their size.  Currently all turbines are set to be the same size, but this can
easily be changed (only the input needs to be rewritten).  Several common 
orientations (aligned, staggered, etc) are available and more can be added by 
the user.

The second group of settings relates to the turbine model.  The user is able to 
specify the thrust coefficients Ct and Ct' as well as the time scale for the 
exponential filter, T_avg_dim.  

The third group of settings relates to the filtering of the indicator function.
Finally, the user can choose to use the output from a previous simulation 
(namely the file "turbine/turbine_u_d_T.dat") to continue during a new run by 
setting turbine_cumulative_time = .true.

--------------------------------------------------------------------------------
                            TURBINE OUTPUTS
--------------------------------------------------------------------------------

All output files relating to the turbines can be found in the "turbine" folder.
The indicator function is written to "nodes_filtered*" and "nodes_unfiltered*"
Tecplot-formatted files.  It can be useful to load these files with data output
to visualize the turbines, but this can be tricky with Tecplot (it requires
interpolation between zones).

The following quantities are also written to file for each turbine:
    current time
    instantaneous disk-averaged velocity
    current time- and disk-averaged velocity
    instantaneous total force for this turbine
    instantaneous power for this turbine
in the files "turbine_#_forcing.dat".

Finally, the values of the time- and disk-averaged velocity for each turbine as
well as the filtering time scale are written to file "turbine_u_d_T.dat".  
These are needed to continue a simulation (turbine_cumulative_time flag).

================================================================================
                            OTHER NOTES
================================================================================

1. The averaged values in io.f90 are averaged across the y-direction
   for each x-location in each z-plane. 

2. ke is the average of the domain

3. Due to the FFT algorithm performed, Nx and Ny must be positive 
   integers of 2; no limitation exists on Nz

4. The mesh is staggered in the vertical direction with w stored between
   the other variables.

5. Albertsons dissertation mentions that it currently uses 2nd order finite differencing
   but plans on a future implementation of Chebyshev's method with preconditioning.

6. Value arrays (don't know a better name at the time) are being used to 
   read in the information from trees.conf. It may be more advantageous
   to put this information in a namelist and read directly. The function
   'case' is used to grab the information from the conf file.

7. "The surface shear stress is computed over the bottom boundary as a function
    of local roughness length (zo)", or z_i as used in the code, "and near surface
    resolved velocity by integrating the logarithmic velocity gradient from z=zo
    to the height of the first node above the wall" Albertson

8. Structures larger than Δx are explicitly calculated (resolved
    scales). Structures smaller than Δx must be filtered out (subgrid scales),
    formally known as low-pass filtering.

9.  Reynolds averaging:  
     a. split variables in mean part and fluctuation
     b. spatially average the model equations

10. The filter procedure removes the small scales from the model
    equations, but it produces new unknowns, mainly averages of 
    fluctuation products. These unknowns have to be parameterized 
    using information from the resolved scales.

